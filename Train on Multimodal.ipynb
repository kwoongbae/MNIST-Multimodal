{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2c1afea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리들 불러오기\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Model\n",
    "from keras import layers\n",
    "from keras import Input\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.random.set_seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "333ddf70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00b63149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1)\n",
      "(10000, 28, 28, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-13 13:46:14.202196: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-08-13 13:46:14.568505: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22360 MB memory:  -> device: 0, name: NVIDIA RTX A5000, pci bus id: 0000:17:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# cnn_input\n",
    "\n",
    "(x_image_train, _) , (x_image_test, _) = keras.datasets.mnist.load_data()\n",
    "x_image_train = x_image_train.astype(\"float32\") / 255\n",
    "x_image_test = x_image_test.astype(\"float32\") / 255\n",
    "\n",
    "x_image_train = np.expand_dims(x_image_train, -1)\n",
    "x_image_test = np.expand_dims(x_image_test, -1)\n",
    "\n",
    "sample = layers.GaussianNoise(0.9)\n",
    "x_image_train = sample(x_image_train, training = True)\n",
    "x_image_test = sample(x_image_test, training = True)\n",
    "\n",
    "print(x_image_train.shape)\n",
    "print(x_image_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6f9ecae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 472)\n",
      "(10000, 472)\n"
     ]
    }
   ],
   "source": [
    "x_sequence_train_tf = np.load('../x_sequence_train_noise_ver3.npy')\n",
    "x_sequence_test_tf = np.load('../x_sequence_test_noise_ver3.npy')\n",
    "\n",
    "print(x_sequence_train_tf.shape)\n",
    "print(x_sequence_test_tf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48184e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정답지 생성\n",
    "train_label_textfile = pd.read_csv('../trainlabels.txt',index_col=False)\n",
    "train_label = np.array(train_label_textfile)\n",
    "\n",
    "test_label_textfile = pd.read_csv('../testlabels.txt',index_col=False)\n",
    "test_label = np.array(test_label_textfile)\n",
    "\n",
    "y_train = tf.keras.utils.to_categorical(train_label, num_classes=10)\n",
    "y_test = tf.keras.utils.to_categorical(test_label, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "527fad84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프 그려주는 함수\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def graph(history):\n",
    "    acc = history.history['acc']\n",
    "    val_acc = history.history['val_acc']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    \n",
    "    epochs = range(1,len(acc)+1)\n",
    "    \n",
    "    plt.plot(epochs, acc, 'r',label = 'Training acc')\n",
    "    plt.plot(epochs, val_acc, 'b',label = 'Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    \n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849feb21",
   "metadata": {},
   "source": [
    "## 3-1 CNN1 + LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55e86bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnn model 구성\n",
    "image_input = Input(shape = (28,28,1), dtype = 'float32', name = 'image_mnist')\n",
    "\n",
    "x = layers.Conv2D(32, kernel_size = (3,3), activation = 'relu')(image_input)\n",
    "x = layers.MaxPooling2D(pool_size = (2,2))(x)\n",
    "x = layers.Conv2D(64, kernel_size = (3,3), activation = 'relu')(x)\n",
    "x = layers.MaxPooling2D(pool_size = (2,2))(x)\n",
    "x = layers.Flatten()(x)\n",
    "image_output = layers.Dropout(0.5)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19bba0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rnn model 구성\n",
    "\n",
    "sequence_input = Input(shape = (472), dtype = 'int32', name = 'sequence_mnist')\n",
    "y = layers.Embedding(512,128)(sequence_input)\n",
    "sequence_output = layers.LSTM(128)(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29883378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 연결합니다.\n",
    "concatenated = layers.concatenate([image_output, sequence_output])\n",
    "\n",
    "# softmax 분류기를 추가합니다.\n",
    "answer = layers.Dense(10, activation = 'softmax')(concatenated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c3e1172",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3_1 = Model([image_input, sequence_input], answer)\n",
    "model3_1.compile(optimizer = 'Adam', loss = 'categorical_crossentropy', metrics = ['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a23a1a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "image_mnist (InputLayer)        [(None, 28, 28, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 26, 26, 32)   320         image_mnist[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 13, 13, 32)   0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 11, 11, 64)   18496       max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 5, 5, 64)     0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "sequence_mnist (InputLayer)     [(None, 472)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 1600)         0           max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 472, 128)     65536       sequence_mnist[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 1600)         0           flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 128)          131584      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 1728)         0           dropout[0][0]                    \n",
      "                                                                 lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 10)           17290       concatenate[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 233,226\n",
      "Trainable params: 233,226\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model3_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e31c749",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-13 13:46:15.478480: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-13 13:46:17.059074: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8201\n",
      "2022-08-13 13:46:19.802589: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375/375 [==============================] - 18s 32ms/step - loss: 1.0057 - acc: 0.6637 - val_loss: 0.5475 - val_acc: 0.8273\n",
      "Epoch 2/80\n",
      "375/375 [==============================] - 12s 32ms/step - loss: 0.6072 - acc: 0.7970 - val_loss: 0.4627 - val_acc: 0.8497\n",
      "Epoch 3/80\n",
      "375/375 [==============================] - 12s 32ms/step - loss: 0.5468 - acc: 0.8185 - val_loss: 0.4362 - val_acc: 0.8559\n",
      "Epoch 4/80\n",
      "375/375 [==============================] - 11s 31ms/step - loss: 0.5205 - acc: 0.8287 - val_loss: 0.4234 - val_acc: 0.8624\n",
      "Epoch 5/80\n",
      "375/375 [==============================] - 12s 31ms/step - loss: 0.5048 - acc: 0.8307 - val_loss: 0.4090 - val_acc: 0.8661\n",
      "Epoch 6/80\n",
      "375/375 [==============================] - 12s 31ms/step - loss: 0.4919 - acc: 0.8349 - val_loss: 0.4002 - val_acc: 0.8705\n",
      "Epoch 7/80\n",
      "375/375 [==============================] - 12s 32ms/step - loss: 0.4825 - acc: 0.8393 - val_loss: 0.4022 - val_acc: 0.8717\n",
      "Epoch 8/80\n",
      "222/375 [================>.............] - ETA: 4s - loss: 0.4748 - acc: 0.8407"
     ]
    }
   ],
   "source": [
    "history3_1 = model3_1.fit([x_image_train, x_sequence_train_tf], y_train, epochs = 80, batch_size = 128, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86cb3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph(history3_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9183c021",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3_1.evaluate([x_image_test, x_sequence_test_tf], y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b34d5a5",
   "metadata": {},
   "source": [
    "## 3-2. CNN2 + biLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e2e09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnn model 구성 with lenet\n",
    "image_input = Input(shape = (28,28,1), dtype = 'float32', name = 'image_mnist')\n",
    "\n",
    "x = layers.Conv2D(20, kernel_size = 5, padding = 'same')(image_input)\n",
    "x = layers.Activation(\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size = (2,2), strides = (2,2))(x)\n",
    "x = layers.Conv2D(50, kernel_size = 5, padding = 'same')(x)\n",
    "x = layers.Activation(\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size = (2,2), strides = (2,2))(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(500)(x)\n",
    "x = layers.Activation(\"relu\")(x)\n",
    "image_output = layers.Dropout(0.5)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb342c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "  def __init__(self, units):\n",
    "    super(BahdanauAttention, self).__init__()\n",
    "    self.W1 = layers.Dense(units)\n",
    "    self.W2 = layers.Dense(units)\n",
    "    self.V = layers.Dense(1)\n",
    "\n",
    "  def call(self, values, query): # 단, key와 value는 같음\n",
    "    # query shape == (batch_size, hidden size)\n",
    "    # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "    # score 계산을 위해 뒤에서 할 덧셈을 위해서 차원을 변경해줍니다.\n",
    "    hidden_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "    # score shape == (batch_size, max_length, 1)\n",
    "    # we get 1 at the last axis because we are applying score to self.V\n",
    "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "    score = self.V(tf.nn.tanh(\n",
    "        self.W1(values) + self.W2(hidden_with_time_axis)))\n",
    "\n",
    "    # attention_weights shape == (batch_size, max_length, 1)\n",
    "    attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "    # context_vector shape after sum == (batch_size, hidden_size)\n",
    "    context_vector = attention_weights * values\n",
    "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "    return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f678e749",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_input_lstm = Input(shape = (118*4), dtype = 'int32', name = 'sequence_mnist_lstm')\n",
    "y = layers.Embedding(512,32)(sequence_input_lstm)\n",
    "lstm = layers.Bidirectional(layers.LSTM(32, dropout=0.5, return_sequences = True))(y)\n",
    "lstm, forward_h, forward_c, backward_h, backward_c = layers.Bidirectional(layers.LSTM(64, dropout=0.5, return_sequences=True, return_state=True))(lstm)\n",
    "\n",
    "state_h = layers.Concatenate()([forward_h, backward_h])\n",
    "state_c = layers.Concatenate()([forward_c, backward_c])\n",
    "\n",
    "attention = BahdanauAttention(64) # 가중치 크기 정의\n",
    "context_vector, attention_weights = attention(lstm, state_h)\n",
    "\n",
    "# 1. Dense\n",
    "# 2. BN\n",
    "# 3. Activation\n",
    "dense = layers.Dense(20)(context_vector)\n",
    "bn = layers.BatchNormalization()(dense)\n",
    "activation = layers.Activation(activation = 'relu')(bn)\n",
    "\n",
    "# BN\n",
    "#BN = layers.BatchNormalization()(context_vector)\n",
    "#dense1 = layers.Dense(20, activation=\"relu\")(BN)\n",
    "\n",
    "sequence_output = layers.Dropout(0.5)(activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6019c756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 연결합니다.\n",
    "concatenated = layers.concatenate([image_output, sequence_output])\n",
    "\n",
    "# softmax 분류기를 추가합니다.\n",
    "answer = layers.Dense(10, activation = 'softmax')(concatenated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a929c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3_2 = Model([image_input, sequence_input_lstm], answer)\n",
    "model3_2.compile(optimizer = 'Adam', loss = 'categorical_crossentropy', metrics = ['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54e807a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b288836",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history3_2 = model3_2.fit([x_image_train, x_sequence_train_tf], y_train, epochs = 80, batch_size = 128, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cb0042",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph(history3_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11502920",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3_2.evaluate([x_image_test, x_sequence_test_tf], y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ce2727",
   "metadata": {},
   "source": [
    "## 3-3. CNN3 + Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eace1deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnn model 구성 with lenet\n",
    "image_input = Input(shape = (28,28,1), dtype = 'float32', name = 'image_mnist')\n",
    "\n",
    "x = layers.Conv2D(20, kernel_size = 5, padding = 'same')(image_input)\n",
    "x = layers.Activation(\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size = (2,2), strides = (2,2))(x)\n",
    "x = layers.Conv2D(50, kernel_size = 5, padding = 'same')(x)\n",
    "x = layers.Activation(\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size = (2,2), strides = (2,2))(x)\n",
    "x = layers.Conv2D(100, kernel_size = 5, padding = 'same')(x)\n",
    "x = layers.Activation(\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size = (2,2), strides = (2,2))(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(500)(x)\n",
    "x = layers.Activation(\"relu\")(x)\n",
    "x = layers.Dense(1000)(x)\n",
    "x = layers.Activation(\"relu\")(x)\n",
    "x = layers.Dense(250)(x)\n",
    "x = layers.Activation(\"relu\")(x)\n",
    "image_output = layers.Dropout(0.5)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95365405",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0501b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff7d913",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 32  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "maxlen = 118*4\n",
    "vocab_size = 1000\n",
    "\n",
    "sequence_input_lstm = layers.Input(shape=(maxlen,))\n",
    "embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "x = embedding_layer(sequence_input_lstm)\n",
    "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "x = transformer_block(x)\n",
    "x = transformer_block(x)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "x = layers.Dense(20, activation=\"relu\")(x)\n",
    "sequence_output = layers.Dropout(0.1)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce33d581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 연결합니다.\n",
    "concatenated = layers.concatenate([image_output, sequence_output])\n",
    "\n",
    "# softmax 분류기를 추가합니다.\n",
    "answer = layers.Dense(10, activation = 'softmax')(concatenated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3ab706",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3_3 = Model([image_input, sequence_input_lstm], answer)\n",
    "model3_3.compile(optimizer = 'Adam', loss = 'categorical_crossentropy', metrics = ['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b54772",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49319a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "history3_3 = model3_3.fit([x_image_train, x_sequence_train_tf], y_train, epochs = 80, batch_size = 128, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8ca08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph(history3_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962ba0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3_3.evaluate([x_image_test, x_sequence_test_tf], y_test, verbose=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
